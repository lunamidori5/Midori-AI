Programs to support:
- Modular Compatibility (Midori AI Docker Subsystem)

- LLM Hosts:
    - Ollama (Docker)
    - Oobabooga (Docker)
    - LocalAI (Docker)

- Photo AI Hosts:
    - InvokeAI (Docker)
    - Automatic1111 (Docker?) | (Apply fix for extension install crashing due to file system permissions)

- LLM FineTuning Hosts:
    - Axolotl (Docker)

- Other Tools:
    - AnythingLLM (Docker)
    - Home Assistant (Docker or VM)
    - ChromaDB for LLM memory applications

Features
- Add file already exists check. (prompt user prior to overwriting. if overwriting is denied, prompt user for the model "name" to set and then set yaml filename to model "name")
- User Control / AI Control

UI Frameworks
- React
- No UI
Notes:
Id love to add a chat room type of webui, but I am not sure how id want to do that...

AI Frameworks
- Google Gem EX (OpenAPI)
- Carly V3 (Autogen / OpenAI)
- OpenAI (Autogen / OpenAI)

AI QNA

Q: What are the advantages and disadvantages of using Docker for LocalAI, InvokeAI, AnythingLLM, Ollama, and Oobabooga?
A: Being able to control them with out inpacting the users computer

Q: How does LM Studio differ from the other programs in terms of its deployment method?
A: It does not offer docker support

Q: What are the pros and cons of using React as a UI framework for these programs?
A: I am not sure, some help on this would be lovely, I know python really well tho... but I would like to make it look really nice

Q: How does Google Gem EX differ from Carly V3 in terms of its capabilities and interoperability?
A: Carly is the better AI, but she is slower and runs on my local servers. Id like to get some more servers to run her.
Google Gem would be great use of this as other than the super hard to hit rate limit, I dont really have much risk
